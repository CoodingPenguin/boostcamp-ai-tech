---
title: â›º DAY 07. ê²½ì‚¬í•˜ê°•ë²•
date: 2021-01-26 11:59:00
category: "â›º Boostcamp"
thumbnail: "./img/7-thumbnail.png"
draft: false
---

![thumbnail](./img/7-thumbnail.png)

> ğŸ™Œì€ **QnAì— ìˆëŠ” ì§ˆë¬¸-ë‹µë³€**ì„ í†µí•´ ì–»ì€ ì§€ì‹ì„ í‘œì‹œí•©ë‹ˆë‹¤.

## [ğŸ‘‰ í”¼ì–´ ì„¸ì…˜](https://github.com/boostcamp-ai-tech-4/peer-session/issues/26)

### ì§ˆë¬¸ & ê¿€íŒ

- [[í­ê·„] íŠ¹ì • axis ë°©í–¥ì—ì„œì˜ í–‰ë ¬ ì—°ì‚°](https://github.com/boostcamp-ai-tech-4/peer-session/issues/20)
- [[íˆìŠ¤] ë²¡í„°ê°„ ìœ ì‚¬ë„ì™€ ë‚´ì ](https://github.com/boostcamp-ai-tech-4/peer-session/issues/21)
- [[í›„ë¯¸] ë¶€ë™ ì†Œìˆ˜ì  ì²˜ë¦¬](https://github.com/boostcamp-ai-tech-4/peer-session/issues/22)

### ê¸°ë¡

- ì €ë²ˆ ì£¼ì˜ íŒŒì´ì¬ ë¬¸ë²•ì´ë¡ ì´ ëë‚˜ê³  ë”¥ëŸ¬ë‹ì„ ë§›ë³´ê¸° ì‹œì‘í–ˆëŠ”ë° **ìˆ˜ì‹**ì„ ë³´ë‹ˆ ë˜ê²Œ ìƒˆë¡­ë‹¤ã…‹ã…‹ ê²°êµ­ ê³ ë“±í•™êµ, ëŒ€í•™êµ ë•Œ ì •ë¦¬í•œ ìˆ˜í•™ ë…¸íŠ¸ë¥¼ êº¼ë‚´ì„œ ë‹¤ì‹œ ë³´ê³  ìˆë‹¤..ã„¸ã„¹ã„¹
- ì˜¤ëŠ˜ ê°•ì˜ì—ì„œ ìˆ˜ì‹ì´ ì •ë§ ë§ì´ ë‚˜ì™”ëŠ”ë° ê·¸ë˜ì„œ ê·¸ëŸ°ì§€ í”¼ì–´ì„¸ì…˜ ë•Œë„ ìˆ˜ì‹ê´€ë ¨ ì§ˆë¬¸ì´ ë§ì´ ë‚˜ì™”ë‹¤. ë§ì´ í—·ê°ˆë ¤ì„œ ë‹µì„ ëª»í–ˆëŠ”ë° ê·¸ë˜ì„œ í•™ìŠµ ì •ë¦¬ë¥¼ **ì¡°ê¸ˆ ë” ê¼¼ê¼¼íˆ** í–ˆë‹¤.
- ì˜¤ëŠ˜ ì²« TMI ìê¸°ì†Œê°œë¥¼ í–ˆë‹¤! íŒ€ì›ì˜ ìƒˆë¡œìš´ ë©´ëª¨(?)ë¥¼ ë³¼ ìˆ˜ ìˆëŠ” ì¢‹ì€ ì‹œê°„ì´ì—ˆë‹¤. ë‹¤ìŒ ë¶„ë„ ê¸°ëŒ€ê°€ ëœë‹¤.
- ë‹¤ë¥¸ íŒ€ì˜ í”¼ì–´ì„¸ì…˜ ê¸°ë¡ì„ ìš°ì—°íˆ ë³´ê²Œ ë˜ì—ˆëŠ”ë° ê·¸ íŒ€ì€ [KPT(Keep-Problem-Try)](https://woowabros.github.io/experience/2020/05/13/birth-of-team-culture.html) íšŒê³ ë¥¼ í–ˆë‹¤. ì •ë§ ì¢‹ì€ ë°©ì‹ì´ë¼ê³  ìƒê°í•´ ì´ë²ˆ ì£¼ ë§ˆì§€ë§‰ ë‚  ë‚˜ë„ ì´ ë°©ì‹ì— ë§ì¶°ì„œ íšŒê³ ë¥¼ í•´ë³¼ ì˜ˆì •ì´ë‹¤.

## Table of Contents

- [ì¼ë³€ìˆ˜ í•¨ìˆ˜ì˜ ë¯¸ë¶„](#ì¼ë³€ìˆ˜-í•¨ìˆ˜ì˜-ë¯¸ë¶„)
- [ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ë¯¸ë¶„](#ë‹¤ë³€ìˆ˜-í•¨ìˆ˜ì˜-ë¯¸ë¶„)
- [ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ì„ í˜•íšŒê·€ ê³„ìˆ˜ êµ¬í•˜ê¸°](#ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ-ì„ í˜•íšŒê·€-ê³„ìˆ˜-êµ¬í•˜ê¸°)
- [í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²• (SGD)](#í™•ë¥ ì -ê²½ì‚¬í•˜ê°•ë²•-sgd)
- [References](#references)

## ì¼ë³€ìˆ˜ í•¨ìˆ˜ì˜ ë¯¸ë¶„

$$
f'(x) = \lim_{h \rightarrow 0}\frac{f(x+h) - f(x)}{h}
$$

- **ë³€ìˆ˜ì˜ ì›€ì§ì„ì— ë”°ë¥¸ í•¨ìˆ˜ê°’ì˜ ë³€í™”**ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•œ ë„êµ¬
- í•¨ìˆ˜ $f$ì˜ ì£¼ì–´ì§„ ì  $(x, f(x))$ì—ì„œì˜ **ì ‘ì„ ì˜ ê¸°ìš¸ê¸°**

### ê²½ì‚¬í•˜ê°•ë²•ê³¼ ê²½ì‚¬ìƒìŠ¹ë²•

![ë¯¸ë¶„ê°’ì„ ë”í•˜ê±°ë‚˜ ë¹¼ë©´](./img/7-differentiation.png)

ìœ„ì˜ ê·¸ë¦¼ì„ ë³´ë©´ $f(x)$ì™€ ìƒê´€ ì—†ì´ í•¨ìˆ˜ì— ë¯¸ë¶„ê°’ì„ ë”í•˜ë©´ í•¨ìˆ˜ê°’ì´ ì¦ê°€í•˜ê³ , ë¯¸ë¶„ê°’ì„ ë¹¼ë©´ ê°ì†Œí•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ ë•Œ ì „ìë¥¼ **ê²½ì‚¬ìƒìŠ¹ë²• (Gradient Ascent)**, í›„ìë¥¼ **ê²½ì‚¬í•˜ê°•ë²• (Gradient Descent)**ë¼ê³  í•œë‹¤.

- ì´ ë‘ ë°©ë²•ì„ í†µí•´ í•¨ìˆ˜ì˜ ê·¹ëŒ€ê°’/ê·¹ì†Œê°’ì˜ ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤.
- ê·¹ëŒ€ê°’/ê·¹ì†Œê°’ì— ë„ë‹¬í•˜ë©´ ì¦‰, ë¯¸ë¶„ê°’ì´ 0ì´ ë˜ë©´ ì—…ë°ì´íŠ¸ë¥¼ ë©ˆì¶˜ë‹¤.

### ì¼ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê²½ì‚¬í•˜ê°•ë²• êµ¬í˜„

ë‹¤ìŒì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•´ë³´ë©´ ë¯¸ë¶„ê°’ì´ `epsilon`ë³´ë‹¤ ì‘ì•„ì§ˆ ë•Œê¹Œì§€ í•¨ìˆ˜ê°’ì„ ì—…ë°ì´íŠ¸ ì‹œí‚¤ê³  ë‹¤ì‹œ ë¯¸ë¶„ê°’ì„ êµ¬í•˜ëŠ” ê²ƒì„ ë°˜ë³µí•œë‹¤.

- í•„ìš”í•œ í•¨ìˆ˜ ì •ì˜

```python
def func(val):
    fun = sym.poly(x**2 + 2*x + 3)
    return fun.subs(x, val), fun    # xë¥¼ valë³€ìˆ˜ë¡œ ëŒ€ì²´

def func_gradient(fun, val):
    _, function = fun(val)
    diff = sym.diff(function, x)    # xì— ëŒ€í•´ í•¨ìˆ˜ ë¯¸ë¶„
    return diff.subs(x, val), diff  # xë¥¼ valë¡œ ëŒ€ì²´

def gradient_descent(fun, init_point, lr_rate=1e-2, epsilon=1e-5):
    cnt = 0
    val = init_point
    diff, _ = func_gradient(fun, init_point) # ì²« ë¯¸ë¶„
    while np.abs(diff) > epsilon:
        val = val - lr_rate * diff           # í•¨ìˆ˜ ì—…ë°ì´íŠ¸
        diff,  _ = func_gradient(fun, val)   # ë‹¤ì‹œ ë¯¸ë¶„
        cnt += 1
        print(f"[{cnt:^3}] val: {val:<10.5f}, diff: {diff:<10.5f}")
    print(f"í•¨ìˆ˜: {fun(val)[1]}, ì—°ì‚°íšŸìˆ˜: {cnt}, ìµœì†Œì : ({val}, {fun(val)[0]})")
```

- ì„ì˜ì˜ ë°ì´í„° ìƒì„± ë° ê²½ì‚¬í•˜ê°•ë²• ìˆ˜í–‰

```python
gradient_descent(fun=func, init_point=np.random.uniform(-2, 2))
```

## ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ë¯¸ë¶„

### ë³€ìˆ˜ê°€ ë²¡í„°ì¸ ê²½ìš°

ì•ì„  ê²½ìš°ëŠ” ë³€ìˆ˜ê°€ $x$ ì¦‰, í•˜ë‚˜ì¸ ê²½ìš°ì¼ ë•Œì˜ ë¯¸ë¶„ì´ë‹¤. ê·¸ëŸ¼ ë³€ìˆ˜ê°€ $x, y, z$ ì—¬ëŸ¬ ê°œì¸ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê²½ìš° ì–´ë–»ê²Œ ë¯¸ë¶„ì„ í•´ì•¼í• ê¹Œ?
ì—¬ê¸°ì„œ $x, y, z$ë¥¼ ë¬¶ì–´ì„œ ë²¡í„°ë¡œ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë©°, ì´ ë²¡í„°ë¥¼ ì…ë ¥ì„ ë°›ëŠ” ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ëŠ” **í¸ë¯¸ë¶„**ì„ í†µí•´ ë¯¸ë¶„ì„ í•  ìˆ˜ ìˆë‹¤.

$$
\partial_{x_i}f(x) = \lim_{h \rightarrow 0}\frac{f(x+he_i) - f(x)}{h}
$$

ì´ë ‡ê²Œ ê° ë³€ìˆ˜ $x, y, z$ ë³„ë¡œ í¸ë¯¸ë¶„í•œ ê²°ê³¼ë¥¼ ë²¡í„°ë¥¼ ë¬¶ì€ ê²ƒì„ **ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°(gradient vector)**ë¼ê³  í•˜ë©° ì´ë¥¼ ë³€ìˆ˜ ë³€ìˆ˜ $x, y, z$ë¥¼ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë° ì‚¬ìš©í•œë‹¤.

$$
\nabla f = (\partial_{x_1}f, \partial_{x_2}f, ..., \partial_{x_d}f)
$$

<div class="quote-block">
<div class="quote-block__emoji">ğŸ’¡</div>
<div class="quote-block__content" markdown=1>

$e_i$ì˜ ì˜ë¯¸

$i$ë²ˆì§¸ ê°’ë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ëŠ” 0ì¸ ë‹¨ìœ„ë²¡í„°(unit vector)ë¥¼ ë§í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, $e_3 = [0, 0, 1, 0, 0, ...]$ì´ë‹¤.

</div>
</div>

### ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ê²½ì‚¬í•˜ê°•ë²• êµ¬í˜„

ì¼ë³€ìˆ˜ í•¨ìˆ˜ì˜ ì½”ë“œì™€ í¬ê²Œ ë‹¤ë¥´ì§€ ì•Šì§€ë§Œ **ì…ë ¥ê°’ì´ ë²¡í„°ë¼ëŠ” ì **ì„ ì£¼ì˜í•´ì•¼ í•œë‹¤. í•™ìŠµ ì¢…ë£Œì¡°ê±´ ë¶€ë¶„ì„ ë³´ë©´ ì ˆëŒ€ê°’ì¸ ì¼ë³€ìˆ˜ í•¨ìˆ˜ì™€ëŠ” ë‹¬ë¦¬ **Norm**ìœ¼ë¡œ ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ì˜ í¬ê¸°ë¥¼ êµ¬í•´ì•¼í•œë‹¤.

- í•„ìš”í•œ í•¨ìˆ˜ ì •ì˜

```python
def eval_(fun, val):
    val_x, val_y = val                            # ë‹¤ë³€ìˆ˜ x, y
    fun_eval = fun.subs(x, val_x).subs(y, val_y)  # xë¥¼ val_xë³€ìˆ˜ë¡œ, yë¥¼ val_yë¡œ ëŒ€ì²´
    return fun_eval

def func_multi(val):
    x_, y_ = val
    func = sym.poly(x**2 + 2*y**2)      # ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ ì •ì˜
    return eval_(func, [x_, y_]), func

def func_gradient(fun, val):
    x_, y_ = val
    _, function = fun(val)
    diff_x = sym.diff(function, x)      # xì— ëŒ€í•œ í¸ë¯¸ë¶„
    diff_y = sym.diff(function, y)      # yì— ëŒ€í•œ í¸ë¯¸ë¶„
    grad_vec = np.array([eval_(diff_x, [x_, y_]), eval_(diff_y, [x_, y_])], dtype=float)   # ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°
    return grad_vec, [diff_x, diff_y]

def gradient_descent(fun, init_point, lr_rate=1e-2, epsilon=1e-5):
    cnt = 0
    val = init_point
    diff, _ = func_gradient(fun, init_point) # ì²« ë¯¸ë¶„
    while np.linalg.norm(diff) > epsilon:
        val = val - lr_rate * diff           # í•¨ìˆ˜ ì—…ë°ì´íŠ¸
        diff,  _ = func_gradient(fun, val)   # ë‹¤ì‹œ ë¯¸ë¶„
        cnt += 1
        print(f"[{cnt:^3}] val: {str(val):<30}, diff: {str(diff):<30}")
    print(f"í•¨ìˆ˜: {fun(val)[1]}, ì—°ì‚°íšŸìˆ˜: {cnt}, ìµœì†Œì : ({val}, {fun(val)[0]})")
```

- ì„ì˜ì˜ ë°ì´í„° ìƒì„± ë° ê²½ì‚¬í•˜ê°•ë²• ìˆ˜í–‰

```python
pt = [np.random.uniform(-2, 2), np.random.uniform(-2, 2)]
gradient_descent(fun=func_multi, init_point=pt)
```

## ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ì„ í˜•íšŒê·€ ê³„ìˆ˜ êµ¬í•˜ê¸°

ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„° í–‰ë ¬ $X$, íšŒê·€ê³„ìˆ˜ ë²¡í„° $\beta$, ë°ì´í„°ì˜ ê²°ê³¼ê°’ ë²¡í„° $y$ìœ¼ë¡œ ì´ë£¨ì–´ì§„ íšŒê·€ì‹ $X\beta = y$ì´ ìˆë‹¤ê³  í•˜ì.

$$
\left[
  \begin{array}{ccc}
    â”€â”€ & x_{1} & â”€â”€ \\
    â”€â”€ & x_{2} & â”€â”€ \\
       & \vdots&  \\
    â”€â”€ & x_{n} & â”€â”€
  \end{array}
\right]
\left[
  \begin{array}{ccc}
    \beta_1 \\
    \beta_2 \\
     \vdots\\
    \beta_m
  \end{array}
\right]
=
\left[
  \begin{array}{ccc}
    y_1 \\
    y_2 \\
     \vdots\\
    y_n
  \end{array}
\right]
$$

### ëª©ì ì‹

ìš°ë¦¬ëŠ” ìµœëŒ€í•œ ì‹¤ì œ ë°ì´í„°ì— ê°€ê¹Œìš´ ì¦‰, **|ì‹¤ì œê°’-ì˜ˆì¸¡ê°’|**ì´ ê°€ì¥ ì‘ì„ ë•Œì˜ íšŒê·€ê³„ìˆ˜ ë²¡í„° $\beta$ë¥¼ ì°¾ì•„ì•¼ í•œë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì„ í˜•íšŒê·€ì˜ ëª©ì ì‹ì€ $\lVert y - X\beta\rVert_2$ì´ ëœë‹¤.

### $\beta$ì— ëŒ€í•œ í¸ë¯¸ë¶„

ëª©ì ì‹ $\lVert y - X\beta\rVert_2$ì„ $\beta$ì˜ ìš”ì†Œ ì¤‘ í•˜ë‚˜ì¸ $\beta_k$ì— ëŒ€í•´ ë¯¸ë¶„í•´ë³´ì.

- ğŸ™Œì°¸ê³ ë¡œ ì•„ë˜ì˜ ì‹ì€ L2 Normì´ ì•„ë‹ˆë¼ **RMSE**ì´ë‹¤. RMSEëŠ” L2 Normê³¼ ìœ ì‚¬í•œë° ë°ì´í„° ê°œìˆ˜ë§Œí¼ì„ ë‚˜ëˆ ì¤˜ì•¼í•œë‹¤ëŠ” ì°¨ì´ê°€ ìˆë‹¤. êµìˆ˜ë‹˜ ë§ì”€ìœ¼ë¡œëŠ” ê´€ìš©ì ìœ¼ë¡œ RMSEë¥¼ L2 Normì²˜ëŸ¼ ì“´ë‹¤ê³  í•œë‹¤.

$$
\partial_{\beta_k}\lVert y - X\beta\rVert_2 = \partial_{\beta_k}  \{ \frac{1}{n} \sum_{i=1}^{n} (y_i - \sum_{j=1}^{d}X_{ij}\beta_j)^2 \}^{\frac{1}{2}}
= -\frac{X^T_k(y-X\beta)}{n\lVert y-X\beta\rVert_2}
$$

<details markdown="1">
<summary><strong>ğŸ‘€ ìˆ˜ì‹ ìœ ë„ê³¼ì •</strong></summary>

![ìœ ë„ê³¼ì •1](./img/7-partial-differential-1.png)
![ìœ ë„ê³¼ì •2](./img/7-partial-differential-2.png)

</details>

ìœ„ì˜ **ğŸ‘€ ìˆ˜ì‹ ìœ ë„ê³¼ì •**ì„ ë´¤ë‹¤ë©´ íšŒê·€ê³„ìˆ˜ ë²¡í„° $\beta$ì— ëŒ€í•œ ë¯¸ë¶„ì¸ ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ë„ ì¶©ë¶„íˆ ìœ ë„ê°€ ê°€ëŠ¥í•  ê²ƒì´ë‹¤. ê·¸ë ‡ê²Œ êµ¬í•œ ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\nabla_{\beta}\lVert y - X\beta\rVert_2 = (\partial_{\beta_1}\lVert y - X\beta\rVert_2, ... , \partial_{\beta_d} \lVert y - X\beta\rVert_2)
$$

$$
= \left( -\frac{X^T_1(y-X\beta)}{n\lVert y-X\beta\rVert_2}, ... , -\frac{X^T_d(y-X\beta)}{n\lVert y-X\beta\rVert_2} \right)
$$

$$
= -\frac{X^T(y-X\beta)}{n\lVert y-X\beta\rVert_2}
$$

$\lVert y - X\beta \rVert_2$ ëŒ€ì‹  L2 Normì˜ ì œê³±ì¸ $\lVert y - X\beta \rVert_2^2$ì„ ì“°ë©´ ìˆ˜ì‹ì„ ì¢€ ë” ê°„ë‹¨íˆ ë§Œë“¤ ìˆ˜ ìˆë‹¤.

$$
\nabla_\beta \lVert y - X\beta \rVert_2^2 = (\partial_{\beta_1}\lVert y - X\beta\rVert_2^2, ... , \partial_{\beta_d} \lVert y - X\beta\rVert_2^2)
$$

$$
= - \frac{2}{n} X^T (y - X\beta)
$$

ì´ì œ **ëª©ì ì‹ì„ ìµœì†Œí™”í•˜ëŠ” $\beta$ë¥¼ êµ¬í•˜ëŠ” ê²½ì‚¬í•˜ê°•ë²• ì•Œê³ ë¦¬ì¦˜**ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
\beta^{(t+1)} â† \beta^{(t)}+ \frac{2\lambda}{n} X^T ( y- X\beta^{(t)})
$$

### ê²½ì‚¬í•˜ê°•ë²• ê¸°ë°˜ ì„ í˜•íšŒê·€ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„

```python
X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])
y = np.dot(X, np.array([1, 2])) + 3

beta_gd = [10.1, 15.1, -6.5]                  # [1, 2, 3]ì´ ì •ë‹µ
X_ = np.array([np.append(x, [1]) for x in X]) # biasë¥¼ ê°€ì¤‘ì¹˜ì— ë„£ê¸° ìœ„í•´ 1ì„ ì¶”ê°€

for t in range(5000):
    error = y - X_ @ beta_gd
    grad = - np.transpose(X_) @ error  # ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„° ê³„ì‚°
    beta_gd = beta_gd - 0.01 * grad    # íšŒê·€ê³„ìˆ˜ ë²¡í„° ì—…ë°ì´íŠ¸
    print(f"[{t+1:^4}]   grad: {str(grad):<40} beta_gd: {str(beta_gd):<20}")

print(f"\nìµœì¢… beta_gd: {beta_gd}")
```

## í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²• (SGD)

### ê²½ì‚¬í•˜ê°•ë²•ì˜ í•œê³„

ì´ë¡ ì ìœ¼ë¡œ ê²½ì‚¬í•˜ê°•ë²•ì€ **ë¯¸ë¶„ì´ ê°€ëŠ¥í•˜ê³  ë³¼ë¡í•œ(convex) í•¨ìˆ˜**ì— ëŒ€í•´ì„œëŠ” ì ì ˆí•œ í•™ìŠµë¥ ê³¼ í•™ìŠµíšŸìˆ˜ë¥¼ ì„ íƒí–ˆì„ ë•Œ ìˆ˜ë ´ì´ ë³´ì¥ëœë‹¤. í•˜ì§€ë§Œ ë¹„ì„ í˜•íšŒê·€ì™€ ê°™ì´ ëª©ì ì‹ì´ non-convexí•  ê²½ìš° global minimum ì™¸ì— local minimumì´ ì¡´ì¬í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìˆ˜ë ´ì´ ë³´ì¥ë˜ì§€ ì•ŠëŠ”ë‹¤.

![nonconvex](./img/7-nonconvex.png)

> non-convexí•¨ìˆ˜ì˜ ì˜ˆ

### í•´ê²°ì±…: í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•

$$
\theta^{(t+1)} â† \theta{(t)} - \widehat{\nabla_\theta\mathcal{L}}(\theta^{(t)})
$$

$$
E[\widehat{\nabla_\theta\mathcal{L}}] \approx \nabla_\theta\mathcal{L}
$$

í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(stochastic gradient descent)ì€ **ë°ì´í„° 1ê°œ í˜¹ì€ ì¼ë¶€ë¥¼ ê°€ì§€ê³  ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ë²•**ìœ¼ë¡œ, ìœ„ì™€ ê°™ì€ local minimum ë¬¸ì œë¥¼ í•´ê²°í•´ì¤€ë‹¤. ì¦‰, ë³¼ë¡ì´ ì•„ë‹Œ í•¨ìˆ˜ë„ ì´ ë°©ë²•ì„ í†µí•´ ìµœì í™”(optimization)ê°€ ê°€ëŠ¥í•˜ë‹¤.

ê·¸ëŸ¼ ì–´ë–»ê²Œ ê°€ëŠ¥í•œ ê²ƒì¼ê¹Œ? í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(ì¼ëª… SGD)ëŠ” ë°ì´í„° ì¼ë¶€ì— í•´ë‹¹í•˜ëŠ” **ë¯¸ë‹ˆ ë°°ì¹˜**ë¥¼ ë½‘ì•„ ì—…ë°ì´íŠ¸ë¥¼ í•˜ëŠ”ë° <u>ê³„ì† ë¯¸ë‹ˆ ë°°ì¹˜ì˜ êµ¬ì„±ì´ ë°”ë€ë‹¤.</u> ê·¸ë˜ì„œ í•œ epoch ì „ì— local minimumì— ë¹ ì¡Œë”ë¼ë„ ë‹¤ìŒ epoch ë•Œ êµ¬ì„±ì´ ë°”ë€Œì–´ ê¸°ìš¸ê¸°ê°€ ë‹¤ë¼ì§€ë©´ì„œ local minumumì—ì„œ ë¹ ì ¸ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤.

ğŸ™Œ**ê·¸ëŸ¼ global minimumì—ì„œë„ ë¹„ìŠ·í•œ ì¼ì´ ì¼ì–´ë‚˜ì§€ ì•Šì„ê¹Œ?** ê·¸ëŸ´ ìˆ˜ë„ ìˆë‹¤. SGDì˜ í° ë‹¨ì  ì¤‘ í•˜ë‚˜ê°€ global minimum ê·¼ì²˜ê¹Œì§€ëŠ” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ì§€ë§Œ ì •ì‘ <u>ê·¸ ê·¼ì²˜ì—ì„œëŠ” global minimumì— ë„ë‹¬í•˜ì§€ ëª»í•˜ê³  ì§„ë™í•˜ëŠ” ê²ƒ</u>ì´ë‹¤. ì´ëŸ´ ê²½ìš° ì ì°¨ í•™ìŠµë¥ ì„ ì¤„ì—¬ê°€ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì–´ëŠ ì •ë„ í•´ê²°í•  ìˆ˜ ìˆë‹¤.

ğŸ™Œì°¸ê³ ë¡œ SGDì—ì„œ ë¯¸ë‹ˆë°°ì¹˜ë¥¼ ì‚¬ìš©í•  ë•Œ ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆ ì„ì€ í›„(shuffle) ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë‚˜ëˆ„ê¸° ë•Œë¬¸ì— **í•œ iteration ë‚´ ì¤‘ë³µë˜ëŠ” ì„¸íŠ¸ëŠ” ì¡´ì¬í•˜ì§€ ì•Š**ëŠ”ë‹¤.

### í•˜ë“œì›¨ì–´ì—ì„œ GD vs SGD

ìµœì í™” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ **ê²½ì‚¬í•˜ê°•ë²•(GD)**ë¥¼ ì“°ê³  **ìˆ˜ ë°±ì¥ì˜ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ë°ì´í„°**ë¥¼ ê°€ì§€ê³  í•™ìŠµì„ í•œë‹¤ê³  í•˜ì. GDëŠ” ë¯¸ë¶„ì„ ê³„ì‚°í•  ë•Œ ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ì„œ ë¯¸ë¶„ì„ í•œë‹¤. ì¦‰, ì—°ì‚°ì„ ìœ„í•´ ë°ì´í„°ë¥¼ ëª¨ë‘ ë©”ëª¨ë¦¬ì— ì˜¬ë ¤ì•¼ í•˜ëŠ”ë°, ì´ë¥¼ ì˜¬ë¦¬ë©´ ë¶„ëª…íˆ `ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬`ê°€ ëœ° ê²ƒì´ë‹¤.

ì´ì— ë¹„í•´ **í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(SGD)**ëŠ” ë°ì´í„°ì˜ ì¼ë¶€ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ, GDë³´ë‹¤ëŠ” í›¨ì”¬ ì ì€ ì–‘ì˜ ë©”ëª¨ë¦¬ê°€ ë“¤ê³  í¬ê¸°ê°€ í° ë°ì´í„°ë¼ë„ í•™ìŠµì„ ì‹œí‚¬ ìˆ˜ ìˆë‹¤.

## References

- [ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ìŠ¤ì¿¨ - 4.4. í–‰ë ¬ì˜ ë¯¸ë¶„](https://datascienceschool.net/02%20mathematics/04.04%20%ED%96%89%EB%A0%AC%EC%9D%98%20%EB%AF%B8%EB%B6%84.html)
- [ë‹¤í¬ í”„ë¡œê·¸ë˜ë¨¸ - ë°±í„° ë¯¸ë¶„ê³¼ í–‰ë ¬ ë¯¸ë¶„](https://darkpgmr.tistory.com/141)
- [ê³µëŒì´ì˜ ìˆ˜í•™ì •ë¦¬ë…¸íŠ¸ - ê²½ì‚¬í•˜ê°•ë²•](https://angeloyeo.github.io/2020/08/16/gradient_descent.html)
- [ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ìŠ¤ì¿¨ - ì„ í˜•íšŒê·€ë¶„ì„ì˜ ê¸°ì´ˆ](https://datascienceschool.net/03%20machine%20learning/04.02%20%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D%EC%9D%98%20%EA%B8%B0%EC%B4%88.html?highlight=intercept)
- [Sympy Modules - abc](https://docs.sympy.org/latest/modules/abc.html)
