---
title: ⛺ DAY 36. 경량화 Ⅰ
date: 2021-03-15 00:00:00
category: "⛺ Boostcamp"
thumbnail: "./img/36-thumbnail.png"
draft: false
---

![thumbnail](./img/36-thumbnail.png)

> ✍ 이번 주는 강의는 한 포스트에 필요한 내용을 스크랩하는 식으로 정리합니다.

## [👉 피어 세션](https://github.com/boostcamp-ai-tech-4/peer-session/issues/103)

### 질문

- [[원딜] pth → onnx → pth 순으로 모델을 저장할 때 마지막 pth 파일의 크기가 커지는 이유?](https://github.com/boostcamp-ai-tech-4/peer-session/issues/102)

### 기록

- 오늘은 궁금했던 주제인 **경량화** 첫 날이었다! 우선 경량화에 있어서 기본으로 알아야할 주제를 배운 후 여러 경량화 기법을 배운다. 오늘 강의 주제는 약간 추상적인 개념이어서 질문은 거의 없었다.
- 각 강의마다 5개씩의 **Further Question**이 있는데 그에 대한 이야기를 진행했다.
  - [onnx](https://onnx.ai/)는 pytorch, tensorflow, keras 등을 다른 프레임워크로 변환을 해주는 라이브러리이다. pytorch 모델을 onnx 모델로 변환한 후 다른 프레임워크로 변환하면 233MB 정도의 일정한 크기를 보여줬다. TF Lite와 같은 경량화 모델은 58MB로 훨씬 작은 크기였다. 반면 계산 최적화에 특화된 TensorRT는 466MB로 큰 크기를 가졌다.
  - 여기서 든 의문은 `pytorch - onnx - pytorch`로 변환했을 때는 351MB로 모델의 크기가 다르다는 점이었다. 그래서 이 부분은 [Issue](https://github.com/boostcamp-ai-tech-4/peer-session/issues/102)로 올려서 각자 생각해보기로 했다.
- 과제는 **사람 얼굴에 Landmark를 표시하는 것**이었다. 10×2 크기의 subplot을 그려야 해서 오랜 만에 `matplotlib`를 공부하는 시간을 가졌다.

## Table of Contents

- [가벼운 모델](#가벼운-모델)
- [동전의 뒷면](#동전의-뒷면)
- [가장 적당하게](#가장-적당하게)

## 가벼운 모델

- [Is there a difference between “==” and “is”? - stackoverflow](https://stackoverflow.com/questions/132988/is-there-a-difference-between-and-is)

  - `==`는 같은 값인지, `is`는 같은 객체를 참조하는지를 검사한다.

- [6.10. Comparisions - Python 3.9.2 Documentation](https://docs.python.org/3/reference/expressions.html#comparisons)

  - 비교연산이 연속적으로 이루어지는 경우 `a op1 b op2 c`는 `a op1 b and b op2 c`로 나타낼 수 있다.
  - 그러므로 `x < y > z`도 `x < y and y > z`로 표현할 수 있으므로 가능한 표현이다.

- [머신러닝이 할 수 있는 일 - 한 페이지 머신러닝](https://opentutorials.org/module/3653/22016)

  - 인공지능은 데이터를 가지고 판단을 할 수 있는 **결정기**이다.
  - 어떤 판단을 내리는 것은 추천시스템과 같은 **가벼운 의사결정**부터 암 진단과 같은 **무거운 결정**까지 다양하다.
  - 딥러닝이 발전하기 전에는 모델의 성능이 좋지 않았기 때문에 가벼운 결정만 가능하였지만 발전 이후 정확도가 거의 100%에 가까워지면서 무거운 의사결정가지 가능하게 되었다.

- [Edge Device - Wikipedia](<https://en.wikipedia.org/wiki/Edge_device#:~:text=An%20edge%20device%20is%20a,network%20(WAN)%20access%20devices.>)

  - `장점 1` 인프라를 구축할 수 없는 환경에서 사용이 가능하다는 장점이 있다.
  - `장점 2` 민감한 데이터의 경우 자체적으로 처리를 하여 외부로 세어나가지 않게 한다.
  - `장점 3` 서버가 따로 없는 경우 latency가 없기 때문에 빠르다.

- [PYTORCH 모델을 ONNX으로 변환하고 ONNX 런타임에서 실행하기 - Pytorch](https://tutorials.pytorch.kr/advanced/super_resolution_with_onnxruntime.html)

  - ONNX는 다양한 프레임워크로 모델을 변환해주는 도구를 제공한다. Pytorch모델을 ONNX로 변환하고 Tensorflow, Keras, TFLite 등 다양한 프레임워크로 변환이 가능하다.
  - `pth → onnx → pth`로 변환하면 원래 모델 구조는 같으나 용량이 증가한 것을 볼 수 있다. (정확한 이유는 찾지 못함)

## 동전의 뒷면

- [인공신경망 학습 레시피 (번역) - Taejoon Byun](https://bntejn.medium.com/%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D-%ED%95%99%EC%8A%B5-%EB%A0%88%EC%8B%9C%ED%94%BC-%EB%B2%88%EC%97%AD-70c5e58341ec)
  - 인공지능 라이브러리를 보면 신경망 학습이 마치 일반적인 소프트웨어와 같이 API와 추상화가 가능하다는 느낌을 주지만 실제 그렇지는 않다. 신경망은 훨씬 더 복잡하고 오류가 발생할 경우 원인은 여러 경우의 수가 존재한다. 특히 모델이 복잡할 수록 디버깅하기 더 까다롭다.
  - 그렇다고 신경망을 디버깅하는 법이 존재하지 않는 것은 아니다. 코딩테스트 때 디버깅을 하는 것처럼 극단적인 값들을 넣어보거나 입력이 잘못되었는지 확인하는 등의 방법을 사용할 수 있다. 자세한 방법은 위의 글을 살펴볼 것!
- [Tiny Machine Learning - mit-han-lab](https://hanlab.mit.edu/projects/tinyml/)
  - 작은 기기에 넣기 위해 메모리, 계산량을 효율적으로 줄이고 성능은 기존 모델과 비슷하게 유지시키는 인공지능을 말한다.
  - 주로 클라우드 서버를 거치지 않고 기기 자체에서 정보를 수집하고 연산을 수행하는 On-device AI에서 쓰인다.
  - On-device AI의 장점은 인터넷과 연결되지 않았기 때문에 정보의 Privacy를 보장할 수 있으며, 서버를 거치지 않기 떄문에 매우 적은 Latency가 든다. 인프라가 구축되지 않은 환경에서 사용하기 좋다.

## 가장 적당하게

- [4 Distance Measures for Machine Learning - Machine Learning Mastery](https://machinelearningmastery.com/distance-measures-for-machine-learning/)
- [Different Types of Distance Metrics used in Machine Learning - Kunal Gohrani](https://medium.com/@kunal_gohrani/different-types-of-distance-metrics-used-in-machine-learning-e9928c5e26c7)
  - `유클리드 거리`: 두 벡터 간의 차이의 제곱의 합의 제곱근으로 KNN이나 K-Means에서 벡터의 거리 계산 시 일반적으로 사용하며 L2-Norm 계산 시에도 사용한다.
  - `코사인 거리`: 코사인 유사도라고도 불리며 협업필터링에서 두 벡터의 유사성을 알고 싶을 때 사용한다. 이 값이 작을수록 유사성이 높으며, 같은 방향을 가리킨다.
  - `해밍 거리`: 두 이진 문자열을 비교하기 위한 지표로 사용된다. 예를 들면, 1011과 1111이 있다고 할 때, 두 번째 자리만 바꾸면 같은 문자열이 되므로 두 문자열의 해밍거리는 1이다. 계산 시 XOR연산을 사용하여 구한다.
- [Evaluation & Calculate Top-N Accuracy: Top 1 and Top 5 - stackoverflow](https://stackoverflow.com/questions/37668902/evaluation-calculate-top-n-accuracy-top-1-and-top-5)
  - Top-1 Accuracy는 일반적인 정확도로 모든 예측값이 정답과 일치해야 100% 정확도를 얻는다.
  - Top-5 Accuracy는 모델이 5개의 높은 확률을 가진 예측값을 출력하고 이 예측값 중 하나라도 정답과 일치하기만 하면 된다.
- [Once-for-All: Train One Network and Specialize it for Efficient Deployment](https://arxiv.org/pdf/1908.09791.pdf)
  - 도메인에 따라 비용(Cost)가 달라지며, 자신이 정한 비용에 따라 모델을 경량화 할 수 있다.
  - 예를 들면, 위의 논문은 $CO^2$를 비용함수(Cost Function)으로 두어 학습시켰으며 다른 모델에 비해 계산량은 적음에도 더 좋은 성능을 보여준다.
- [[밑바닥 딥러닝] 여러 함수의 역전파와 오차역전파법 구현 - 코딩하는펭귄의 저장소](../../deep-learning/several-function-back-propagation-and-implementation/#sigmoid-계층)
  - sigmoid함수는 곱셈, 지수함수, 덧셈, 나눗셈 연산을 합한 것이다. 연전파 시 나눗셈부터 거슬러 올라가 국소적 미분을 하여 연쇄법칙을 적용을 하면 $\frac{\partial L}{\partial y}y^2exp(-x)$이 나오고 이 식을 정리하면 $\frac{\partial L}{\partial y}y(1-y)$이 나온다. 자세한 유도방법은 링크를 참고!
